{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 모델 클래스\n",
    " - 부모 클래스 : nn.Module\n",
    " - 필수오버라이딩 : \n",
    "  * __ init __() : 모델 층 구성 => 설계\n",
    "  * foward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈로딩\n",
    "import torch                                # 텐서 관련 모듈\n",
    "import torch.nn as nn                       # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F             # 인공신경망 관련 함수들 모듈(손실함수, 활성화함수 등)\n",
    "import torch.optim as optim                 # 최적화 관련 모듈(가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "from torchinfo import summary               #모델 구조 및 정보 관련 모듈\n",
    "from torchmetrics.regression import *       # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *   # 분류 성능 지표 관련 모듈\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# 텐서 저장 및 실행 위치\n",
    "DEVICE = 'cuda'if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망클래스 <hr>\n",
    "    * 입력층 - 입력 피쳐 고정\n",
    "    * 출력층 - 출력 타겟 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터셋 : 피쳐 4개, 타겟 1개, 회귀\n",
    "# 입력층 : 입력     4개     출력  20개     AF   ReLU\n",
    "# 은닉층 : 입력    20개     출력  100개    AF   ReLU\n",
    "# 출력층 : 입력   100개     출력  1개      AF   Sigmoid & Softmax(분류일 경우 확률로 바꿔줘야 하니 sigmoid 이진분류에는 sigmoid, 다중분류에는(Softmax) 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 입력이 고정적인 모델\n",
    "class MyModel(nn.Module) :\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self) :\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(4,20)          # w 4 + b 1 => 1p, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20,100)       # w 20 + b 1 => 1p, 21 * 100 = 2100개 변수\n",
    "        self.out_layer = nn.Linear(100,1)           # w 100 + b 1 => 1p, 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자(x) : 학습용 데이터\n",
    "    def forward(self, x) :\n",
    "        print('calling foward()')\n",
    "        y = self.input_layer(x)\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        y = self.hidden_layer(y)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        self.out_layer(y)\n",
    "\n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델\n",
    "class MyModel2(nn.Module) :\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features) :\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features,20)          # w 4 + b 1 => 1p, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20,100)       # w 20 + b 1 => 1p, 21 * 100 = 2100개 변수\n",
    "        self.out_layer = nn.Linear(100,1)           # w 100 + b 1 => 1p, 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자(x) : 학습용 데이터\n",
    "    def forward(self, x) :\n",
    "        print('calling foward()')\n",
    "        y = self.input_layer(x)\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        y = self.hidden_layer(y)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        self.out_layer(y)\n",
    "\n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수와 은닉층 퍼셉트론 수가 동적인 모델\n",
    "class MyModel3(nn.Module) :\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features, in_out, h_out) :\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features,in_out)          # w 4 + b 1 => 1p, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(in_out,h_out)       # w 20 + b 1 => 1p, 21 * 100 = 2100개 변수\n",
    "        self.out_layer = nn.Linear(h_out,1)           # w 100 + b 1 => 1p, 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자(x) : 학습용 데이터\n",
    "    def forward(self, x) :\n",
    "        print('calling foward()')\n",
    "        y = self.input_layer(x)\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        y = self.hidden_layer(y)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        self.out_layer(y)\n",
    "\n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module) :\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features, in_out, h_out) :\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features, in_out)          # w 4 + b 1 => 1p, 5 * 20 = 100개 변수\n",
    "        for i in range(5) :\n",
    "            self.hidden_layer = nn.Linear(in_out, h_out) \n",
    "\n",
    " \n",
    "        self.out_layer = nn.Linear(h_out,1)           # w 100 + b 1 => 1p, 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자(x) : 학습용 데이터\n",
    "    def forward(self, x) :\n",
    "        print('calling foward()')\n",
    "        y = self.input_layer(x)\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        y = self.hidden_layer(y)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        self.out_layer(y)\n",
    "\n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 인스턴스 생성\n",
    "m1 = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[-0.0419, -0.0171, -0.1875,  0.1150],\n",
      "        [-0.2861, -0.0882,  0.1938,  0.4693],\n",
      "        [ 0.1178, -0.1696,  0.0479, -0.0560],\n",
      "        [ 0.2041,  0.0573,  0.1959,  0.4849],\n",
      "        [-0.2076, -0.0177,  0.1150, -0.0033],\n",
      "        [-0.0479, -0.4425, -0.4313, -0.4499],\n",
      "        [-0.4892, -0.4657, -0.3788, -0.4510],\n",
      "        [-0.4690,  0.2192,  0.3067,  0.3379],\n",
      "        [ 0.2694,  0.1694,  0.2203, -0.2765],\n",
      "        [ 0.4502, -0.0345,  0.4314,  0.1533],\n",
      "        [ 0.3914,  0.3988, -0.1045, -0.1454],\n",
      "        [ 0.0752, -0.0213,  0.0782,  0.2536],\n",
      "        [-0.3907, -0.0229, -0.3924,  0.4829],\n",
      "        [-0.3517,  0.0956, -0.1366,  0.2842],\n",
      "        [ 0.0017, -0.0503,  0.3660,  0.4567],\n",
      "        [-0.3629, -0.4823,  0.0417,  0.1575],\n",
      "        [ 0.1141,  0.4619,  0.2244, -0.2300],\n",
      "        [-0.3424,  0.3879,  0.4792, -0.2373],\n",
      "        [-0.3200,  0.1750, -0.3576, -0.1210],\n",
      "        [-0.4945,  0.1368, -0.1705, -0.2797]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([-0.3179, -0.1759,  0.2375,  0.0283,  0.4306, -0.1787, -0.1463,  0.4894,\n",
      "        -0.4769, -0.1968, -0.3102,  0.4811,  0.2662, -0.1675, -0.2123,  0.1533,\n",
      "        -0.1690,  0.1552, -0.2559, -0.3094], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[-0.0056,  0.1313,  0.1904,  ..., -0.1052, -0.0663,  0.1507],\n",
      "        [ 0.1910,  0.0779, -0.1677,  ...,  0.2017, -0.1786, -0.2111],\n",
      "        [-0.1209,  0.2146,  0.2012,  ...,  0.1544,  0.1710,  0.0290],\n",
      "        ...,\n",
      "        [-0.1562,  0.1971, -0.1323,  ..., -0.1952,  0.2010,  0.0656],\n",
      "        [ 0.1327, -0.0845, -0.1169,  ...,  0.0673,  0.1587,  0.0027],\n",
      "        [-0.0749,  0.1003,  0.1078,  ..., -0.2007,  0.2111,  0.2181]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([-0.0390, -0.0236,  0.1192,  0.0269, -0.1650,  0.2199,  0.2034, -0.1491,\n",
      "        -0.0603,  0.0131,  0.1591, -0.1326,  0.0578,  0.1851,  0.0272,  0.1220,\n",
      "        -0.1216,  0.1466,  0.0057, -0.0853,  0.0148, -0.1430, -0.1414, -0.0303,\n",
      "        -0.0113, -0.0077,  0.0387,  0.1760, -0.1247,  0.0864,  0.0073, -0.0603,\n",
      "        -0.1842, -0.0229,  0.2157, -0.0417, -0.0980,  0.1780, -0.1587, -0.1294,\n",
      "        -0.0240, -0.0315, -0.1420,  0.1090,  0.1816, -0.0593,  0.0870,  0.0484,\n",
      "         0.0624,  0.1284,  0.0048, -0.0143, -0.0389, -0.1408, -0.1128, -0.0325,\n",
      "        -0.0096,  0.0482,  0.0868, -0.1367,  0.0344,  0.0881, -0.2096,  0.0946,\n",
      "         0.0819, -0.2011, -0.1251,  0.1025,  0.1371,  0.0405,  0.0829, -0.1750,\n",
      "        -0.2150, -0.0576, -0.0798, -0.1159,  0.1812, -0.0866, -0.0155, -0.1115,\n",
      "        -0.1935,  0.0792,  0.1090, -0.1334, -0.1998,  0.1016, -0.1140,  0.2010,\n",
      "         0.0796,  0.1877,  0.1578, -0.0565, -0.0491, -0.0530, -0.1459, -0.1581,\n",
      "         0.1086, -0.1738, -0.0918, -0.0326], requires_grad=True))\n",
      "('out_layer.weight', Parameter containing:\n",
      "tensor([[-0.0687, -0.0103,  0.0824,  0.0187, -0.0712, -0.0302, -0.0409,  0.0033,\n",
      "          0.0062,  0.0528,  0.0666, -0.0856, -0.0452, -0.0231,  0.0605,  0.0484,\n",
      "          0.0469,  0.0108,  0.0655,  0.0324, -0.0381,  0.0742, -0.0379,  0.0209,\n",
      "         -0.0240,  0.0380, -0.0308, -0.0260,  0.0287, -0.0563,  0.0882, -0.0487,\n",
      "          0.0210, -0.0703,  0.0434, -0.0620,  0.0369,  0.0288,  0.0688, -0.0191,\n",
      "         -0.0601, -0.0937, -0.0440, -0.0779, -0.0680, -0.0912,  0.0935,  0.0970,\n",
      "         -0.0403,  0.0022, -0.0144,  0.0475,  0.0162,  0.0555,  0.0412, -0.0262,\n",
      "          0.0376,  0.0567,  0.0926, -0.0972,  0.0486, -0.0611,  0.0144,  0.0581,\n",
      "         -0.0612,  0.0043, -0.0868,  0.0215, -0.0899,  0.0600,  0.0428, -0.0483,\n",
      "          0.0171, -0.0282,  0.0125, -0.0839, -0.0145,  0.0320, -0.0674,  0.0732,\n",
      "         -0.0458,  0.0411,  0.0182, -0.0394, -0.0142,  0.0072,  0.0513, -0.0013,\n",
      "          0.0267,  0.0772, -0.0042,  0.0566, -0.0748,  0.0414, -0.0886, -0.0138,\n",
      "          0.0697,  0.0801,  0.0775,  0.0509]], requires_grad=True))\n",
      "('out_layer.bias', Parameter containing:\n",
      "tensor([-0.0550], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "## 모델 파라미터 => w와 b 확인\n",
    "for m in m1.named_parameters() :print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling foward()\n",
      "tensor([[-0.4545],\n",
      "        [-0.5435]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델 인스턴스명(데이터)\n",
    "# 임의의 데이터 생성\n",
    "dataTS = torch.FloatTensor([[1,3,5,7],[2,4,6,8]])\n",
    "targetTS = torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = m1(dataTS)\n",
    "print(pre_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
